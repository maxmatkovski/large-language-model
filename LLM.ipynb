{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376276ac-2cb0-4743-9b06-6e253a5ddf37",
   "metadata": {},
   "source": [
    "##### Do Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f0332c-09ae-4ad7-b87c-2b9c6c3a9154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed:\n",
    "# !pip install torch\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa7862c-fb38-4958-9c89-f4ce9ce2fbc3",
   "metadata": {},
   "source": [
    "##### Load Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85f3169e-459d-4a32-a888-1e01e5a57a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 438806\n",
      "The Project Gutenberg eBook of Frankenstein; or, the modern prometheus\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before\n"
     ]
    }
   ],
   "source": [
    "with open(\"frankenstein.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Length:\", len(text))\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1fde96-b655-42e8-8e06-dd64179e82a7",
   "metadata": {},
   "source": [
    "##### Trim Gutenberg headers/footers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d76743ca-5151-4442-a13a-abc548e1a5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text length: 419409\n",
      "THE PROJECT GUTENBERG EBOOK FRANKENSTEIN; OR, THE MODERN PROMETHEUS ***\n",
      "\n",
      "Frankenstein;\n",
      "\n",
      "or, the Modern Prometheus\n",
      "\n",
      "by Mary Wollstonecraft (Godwin) Shelley\n",
      "\n",
      "\n",
      " CONTENTS\n",
      "\n",
      " Letter 1\n",
      " Letter 2\n",
      " Letter 3\n",
      " Letter 4\n",
      " Chapter 1\n",
      " Chapter 2\n",
      " Chapter 3\n",
      " Chapter 4\n",
      " Chapter 5\n",
      " Chapter 6\n",
      " Chapter 7\n",
      " Chapter 8\n",
      " Chapter 9\n",
      " Chapter 10\n",
      " Chapter 11\n",
      " Chapter 12\n",
      " Chapter 13\n",
      " Chapter 14\n",
      " Chapter 15\n",
      " Chapter 16\n",
      " Chapter 17\n",
      " Chapter 18\n",
      " Chapter 19\n",
      " Chapter 20\n",
      " Chapter 21\n",
      " Chapter 22\n",
      " Chapter 23\n",
      " Chapter 24\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Letter 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Project Gutenberg texts include legal/licensing text at the start and end.\n",
    "# We want only the actual story, so we trim it.\n",
    "\n",
    "start_marker = \"*** START OF\"  # Marker indicating where the story begins\n",
    "end_marker = \"*** END OF\"      # Marker indicating where the story ends\n",
    "\n",
    "start = text.find(start_marker)  # Find start index of marker\n",
    "end = text.find(end_marker)      # Find end index of marker\n",
    "\n",
    "# If both markers exist, slice the text to only include the story\n",
    "if start != -1 and end != -1:\n",
    "    text = text[start + len(start_marker):end].strip()  # +len(...) skips the marker itself\n",
    "\n",
    "# Print cleaned text length and first 500 characters\n",
    "print(\"Cleaned text length:\", len(text))\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0465a674-6096-4095-bde3-8f4c7a08db33",
   "metadata": {},
   "source": [
    "##### Create character vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b1b1af-9629-4c2d-b594-ddd3a0ead026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 84\n",
      "All unique characters: ['\\n', ' ', '!', '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'æ', 'è', 'é', 'ê', 'ô', '—', '‘', '’', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "# Identify all unique characters in the text\n",
    "# 'set(text)' returns only unique characters\n",
    "# 'list(...)' converts the set to a list so we can index it\n",
    "# 'sorted(...)' ensures a consistent order of characters\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# Total number of unique characters\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Mapping from character → integer index\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "# Mapping from integer index → character\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Helper functions\n",
    "def encode(s):\n",
    "    \"\"\"Convert a string into a list of integer token IDs\"\"\"\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    \"\"\"Convert a list of token IDs back into a string\"\"\"\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "# Inspect vocabulary\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"All unique characters:\", chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8031d9a1-b353-45d2-859b-4622c31f5fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n':0, ' ':1, '!':2, '(':3, ')':4, '*':5, ',':6, '-':7, '.':8, '0':9, '1':10, '2':11, '3':12, '4':13, '5':14, '6':15, '7':16, '8':17, '9':18, ':':19, ';':20, '?':21, 'A':22, 'B':23, 'C':24, 'D':25, 'E':26, 'F':27, 'G':28, 'H':29, 'I':30, 'J':31, 'K':32, 'L':33, 'M':34, 'N':35, 'O':36, 'P':37, 'R':38, 'S':39, 'T':40, 'U':41, 'V':42, 'W':43, 'Y':44, '[':45, ']':46, '_':47, 'a':48, 'b':49, 'c':50, 'd':51, 'e':52, 'f':53, 'g':54, 'h':55, 'i':56, 'j':57, 'k':58, 'l':59, 'm':60, 'n':61, 'o':62, 'p':63, 'q':64, 'r':65, 's':66, 't':67, 'u':68, 'v':69, 'w':70, 'x':71, 'y':72, 'z':73, 'æ':74, 'è':75, 'é':76, 'ê':77, 'ô':78, '—':79, '‘':80, '’':81, '“':82, '”':83\n"
     ]
    }
   ],
   "source": [
    "# Print mappings in a single line, separated by commas\n",
    "print(\", \".join([f\"{repr(ch)}:{i}\" for ch, i in stoi.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "824baa23-9042-4741-9406-63d2950c7cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\n",
      ", 1: , 2:!, 3:(, 4:), 5:*, 6:,, 7:-, 8:., 9:0, 10:1, 11:2, 12:3, 13:4, 14:5, 15:6, 16:7, 17:8, 18:9, 19::, 20:;, 21:?, 22:A, 23:B, 24:C, 25:D, 26:E, 27:F, 28:G, 29:H, 30:I, 31:J, 32:K, 33:L, 34:M, 35:N, 36:O, 37:P, 38:R, 39:S, 40:T, 41:U, 42:V, 43:W, 44:Y, 45:[, 46:], 47:_, 48:a, 49:b, 50:c, 51:d, 52:e, 53:f, 54:g, 55:h, 56:i, 57:j, 58:k, 59:l, 60:m, 61:n, 62:o, 63:p, 64:q, 65:r, 66:s, 67:t, 68:u, 69:v, 70:w, 71:x, 72:y, 73:z, 74:æ, 75:è, 76:é, 77:ê, 78:ô, 79:—, 80:‘, 81:’, 82:“, 83:”\n"
     ]
    }
   ],
   "source": [
    "# Print mappings in a single line, separated by commas\n",
    "print(\", \".join([f\"{repr(ch)}:{i}\" for ch, i in itos.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3bf6253-ff11-48d5-9f0b-1377f46a82a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 token IDs: [40, 29, 26, 1, 37, 38, 36, 31, 26, 24, 40, 1, 28, 41, 40, 26, 35, 23, 26, 38]\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire text into a list of integers manually\n",
    "encoded_text = encode(text)  # use the encode() function we defined earlier\n",
    "\n",
    "# Sanity check\n",
    "print(\"First 20 token IDs:\", encoded_text[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86f455b2-8fb6-4bc2-8986-e97020929fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text: 'THE PROJECT GUTENBER'\n"
     ]
    }
   ],
   "source": [
    "# Convert the first 20 token IDs back to characters\n",
    "decoded_sample = decode(encoded_text[:20])\n",
    "print(\"Decoded text:\", repr(decoded_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9b9f948-c4a4-474a-bfdd-d7159c29e939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 40 -> 'T'\n",
      "ID 29 -> 'H'\n",
      "ID 26 -> 'E'\n",
      "ID  1 -> ' '\n",
      "ID 37 -> 'P'\n",
      "ID 38 -> 'R'\n",
      "ID 36 -> 'O'\n",
      "ID 31 -> 'J'\n",
      "ID 26 -> 'E'\n",
      "ID 24 -> 'C'\n",
      "ID 40 -> 'T'\n",
      "ID  1 -> ' '\n",
      "ID 28 -> 'G'\n",
      "ID 41 -> 'U'\n",
      "ID 40 -> 'T'\n",
      "ID 26 -> 'E'\n",
      "ID 35 -> 'N'\n",
      "ID 23 -> 'B'\n",
      "ID 26 -> 'E'\n",
      "ID 38 -> 'R'\n"
     ]
    }
   ],
   "source": [
    "# Create a list of tuples: (ID, character)\n",
    "id_char_pairs = [(i, itos[i]) for i in encoded_text[:20]]\n",
    "\n",
    "# Print neatly\n",
    "for idx, char in id_char_pairs:\n",
    "    print(f\"ID {idx:2} -> {repr(char)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70249325-7e5e-4da7-b306-d1acee93af61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40:'T' | 29:'H' | 26:'E' | 1:' ' | 37:'P' | 38:'R' | 36:'O' | 31:'J' | 26:'E' | 24:'C' | 40:'T' | 1:' ' | 28:'G' | 41:'U' | 40:'T' | 26:'E' | 35:'N' | 23:'B' | 26:'E' | 38:'R'\n"
     ]
    }
   ],
   "source": [
    "print(\" | \".join([f\"{i}:{repr(itos[i])}\" for i in encoded_text[:20]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e47e3ef-3a34-42d8-aa2c-e7e8bb800849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Input IDs:      [40, 29, 26, 1, 37, 38, 36, 31]\n",
      "Decoded Input:  THE PROJ\n",
      "Target ID:      26\n",
      "Decoded Target: E\n",
      "---\n",
      "Example 2\n",
      "Input IDs:      [29, 26, 1, 37, 38, 36, 31, 26]\n",
      "Decoded Input:  HE PROJE\n",
      "Target ID:      24\n",
      "Decoded Target: C\n",
      "---\n",
      "Example 3\n",
      "Input IDs:      [26, 1, 37, 38, 36, 31, 26, 24]\n",
      "Decoded Input:  E PROJEC\n",
      "Target ID:      40\n",
      "Decoded Target: T\n",
      "---\n",
      "Example 4\n",
      "Input IDs:      [1, 37, 38, 36, 31, 26, 24, 40]\n",
      "Decoded Input:   PROJECT\n",
      "Target ID:      1\n",
      "Decoded Target:  \n",
      "---\n",
      "Example 5\n",
      "Input IDs:      [37, 38, 36, 31, 26, 24, 40, 1]\n",
      "Decoded Input:  PROJECT \n",
      "Target ID:      28\n",
      "Decoded Target: G\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Step: Create Input-Target Sequences\n",
    "# -----------------------------\n",
    "# block_size = number of previous characters the model sees\n",
    "block_size = 8  # small context window for a toy model\n",
    "\n",
    "# Prepare empty lists to store sequences\n",
    "x_manual = []  # input sequences (lists of integers)\n",
    "y_manual = []  # target characters (next character after input)\n",
    "\n",
    "# Loop over the encoded text to create sequences\n",
    "# We stop at len(encoded_text) - block_size to ensure each input sequence\n",
    "# has exactly `block_size` characters\n",
    "for i in range(len(encoded_text) - block_size):\n",
    "    \n",
    "    # Slice the encoded text from i to i+block_size to get the input sequence\n",
    "    # This is the \"context\" the model sees\n",
    "    input_seq = encoded_text[i:i+block_size]\n",
    "    x_manual.append(input_seq)\n",
    "    \n",
    "    # The target is the very next character (token ID) after the input sequence\n",
    "    target = encoded_text[i + block_size]\n",
    "    y_manual.append(target)\n",
    "\n",
    "# -----------------------------\n",
    "# Sanity Check: Print first 5 examples\n",
    "# -----------------------------\n",
    "for i in range(5):\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(\"Input IDs:     \", x_manual[i])          # token IDs of input\n",
    "    print(\"Decoded Input: \", decode(x_manual[i]))  # convert IDs back to characters\n",
    "    print(\"Target ID:     \", y_manual[i])          # token ID of target\n",
    "    print(\"Decoded Target:\", itos[y_manual[i]])    # convert ID to character\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14a98c1a-b407-43b4-a303-aff431688668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (84, 16)\n",
      "Hidden layer weight shape: (128, 32)\n",
      "Output layer weight shape: (32, 84)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Hyperparameters for our toy model\n",
    "# -----------------------------\n",
    "vocab_size = len(chars)     # number of unique characters\n",
    "embedding_dim = 16          # size of each character embedding vector\n",
    "hidden_dim = 32             # size of hidden layer\n",
    "block_size = 8              # context length (number of previous characters)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5a: Initialize Model Parameters\n",
    "# -----------------------------\n",
    "# Embedding matrix: maps token IDs → dense vectors\n",
    "# Shape: vocab_size x embedding_dim\n",
    "# Initialized randomly (small numbers)\n",
    "W_embed = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "\n",
    "# Hidden layer weights: flatten embeddings -> hidden_dim\n",
    "# Shape: (block_size * embedding_dim) x hidden_dim\n",
    "W1 = np.random.randn(block_size * embedding_dim, hidden_dim) * 0.01\n",
    "b1 = np.zeros(hidden_dim)  # bias for hidden layer\n",
    "\n",
    "# Output layer: hidden_dim -> vocab_size logits\n",
    "# Shape: hidden_dim x vocab_size\n",
    "W2 = np.random.randn(hidden_dim, vocab_size) * 0.01\n",
    "b2 = np.zeros(vocab_size)   # bias for output layer\n",
    "\n",
    "# -----------------------------\n",
    "# Sanity Check\n",
    "# -----------------------------\n",
    "print(\"Embedding matrix shape:\", W_embed.shape)\n",
    "print(\"Hidden layer weight shape:\", W1.shape)\n",
    "print(\"Output layer weight shape:\", W2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8106608-49eb-464a-930e-a6f6215d752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax probabilities for a 1D array\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))  # subtract max for numerical stability\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def forward(x_seq):\n",
    "    \"\"\"\n",
    "    Forward pass for a single input sequence.\n",
    "    \n",
    "    x_seq: list of token IDs (length = block_size)\n",
    "    \n",
    "    Returns: \n",
    "        probs: probability distribution over vocab for the next character\n",
    "    \"\"\"\n",
    "    # -----------------------------\n",
    "    # Step 1: Lookup embeddings\n",
    "    # -----------------------------\n",
    "    # For each token ID in x_seq, get its embedding vector\n",
    "    embeds = W_embed[x_seq]  # shape: block_size x embedding_dim\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Step 2: Flatten embeddings into a single vector\n",
    "    # -----------------------------\n",
    "    # So we can feed them into a simple feedforward layer\n",
    "    h_input = embeds.flatten()  # shape: block_size * embedding_dim\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Step 3: Hidden layer with tanh activation\n",
    "    # -----------------------------\n",
    "    # Compute h = tanh(xW + b)\n",
    "    h = np.tanh(h_input @ W1 + b1)  # shape: hidden_dim\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Step 4: Output layer → logits\n",
    "    # -----------------------------\n",
    "    logits = h @ W2 + b2  # shape: vocab_size\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Step 5: Softmax → probabilities\n",
    "    # -----------------------------\n",
    "    probs = softmax(logits)\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "376a99fe-8b88-4665-ba11-b4b1bcd7b83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence (decoded): THE PROJ\n",
      "Target character: E\n",
      "Predicted character (before training): ‘\n",
      "Top 5 predictions:\n",
      "‘: 0.012\n",
      "R: 0.012\n",
      "]: 0.012\n",
      "k: 0.012\n",
      "0: 0.012\n"
     ]
    }
   ],
   "source": [
    "# Take first input sequence\n",
    "x0 = x_manual[0]\n",
    "y0 = y_manual[0]\n",
    "\n",
    "# Forward pass: predicted probability distribution\n",
    "probs = forward(x0)\n",
    "\n",
    "# Predicted character ID (highest probability)\n",
    "pred_id = np.argmax(probs)\n",
    "\n",
    "# Print input, target, and prediction\n",
    "print(\"Input sequence (decoded):\", decode(x0))\n",
    "print(\"Target character:\", itos[y0])\n",
    "print(\"Predicted character (before training):\", itos[pred_id])\n",
    "\n",
    "# Optional: show probabilities of top 5 characters\n",
    "top5 = np.argsort(probs)[-5:][::-1]\n",
    "print(\"Top 5 predictions:\")\n",
    "for i in top5:\n",
    "    print(f\"{itos[i]}: {probs[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c533c586-6ada-41ff-9851-981ac1dd074c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cross_entropy_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 31\u001b[0m\n\u001b[1;32m     26\u001b[0m probs \u001b[38;5;241m=\u001b[39m softmax(logits)                  \u001b[38;5;66;03m# Probabilities over vocab\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Compute Loss (Cross-Entropy)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcross_entropy_loss\u001b[49m(probs, y_true)\n\u001b[1;32m     32\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Gradient of loss w.r.t logits\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cross_entropy_loss' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Hyperparameters\n",
    "# -----------------------------\n",
    "learning_rate = 0.1       # step size for gradient descent\n",
    "num_epochs = 5            # number of passes over the dataset\n",
    "sample_every = 2000       # how often to print generated text\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Loop over each input-target pair\n",
    "    for i in range(len(x_manual)):\n",
    "        x_seq = x_manual[i]    # input sequence of token IDs\n",
    "        y_true = y_manual[i]   # target token ID\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Forward Pass\n",
    "        # -----------------------------\n",
    "        embeds = W_embed[x_seq]                  # Lookup embeddings: block_size x embedding_dim\n",
    "        h_input = embeds.flatten()               # Flatten embeddings to a single vector\n",
    "        h = np.tanh(h_input @ W1 + b1)           # Hidden layer activation: shape hidden_dim\n",
    "        logits = h @ W2 + b2                     # Output layer logits: shape vocab_size\n",
    "        probs = softmax(logits)                  # Probabilities over vocab\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Compute Loss (Cross-Entropy)\n",
    "        # -----------------------------\n",
    "        loss = cross_entropy_loss(probs, y_true)\n",
    "        total_loss += loss\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Backpropagation\n",
    "        # -----------------------------\n",
    "        \n",
    "        # Gradient of loss w.r.t logits\n",
    "        dlogits = probs.copy()\n",
    "        dlogits[y_true] -= 1  # derivative of cross-entropy with softmax\n",
    "        \n",
    "        # -------- Gradients for output layer --------\n",
    "        dW2 = np.outer(h, dlogits)   # hidden_dim x vocab_size\n",
    "        db2 = dlogits                # vocab_size\n",
    "        \n",
    "        # -------- Gradients for hidden layer --------\n",
    "        dh = dlogits @ W2.T                    # propagate gradient to hidden layer: hidden_dim\n",
    "        dh_raw = dh * (1 - h**2)               # tanh derivative\n",
    "        \n",
    "        dW1 = np.outer(h_input, dh_raw)        # block_size*embedding_dim x hidden_dim\n",
    "        db1 = dh_raw                           # hidden_dim\n",
    "        \n",
    "        # -------- Gradients for embeddings --------\n",
    "        dembed_flat = dh_raw @ W1.T            # block_size*embedding_dim\n",
    "        dembed = dembed_flat.reshape(block_size, embedding_dim)  # reshape\n",
    "        dW_embed = np.zeros_like(W_embed)\n",
    "        for j, idx in enumerate(x_seq):\n",
    "            dW_embed[idx] += dembed[j]        # accumulate gradients for each token ID\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Update Weights\n",
    "        # -----------------------------\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "        W_embed -= learning_rate * dW_embed\n",
    "        \n",
    "    # -----------------------------\n",
    "    # End of epoch\n",
    "    # -----------------------------\n",
    "    avg_loss = total_loss / len(x_manual)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Sanity Check: Generate Sample Text\n",
    "    # -----------------------------\n",
    "    start_idx = np.random.randint(0, len(x_manual))\n",
    "    generated_seq = x_manual[start_idx].copy()\n",
    "    print(\"Sample start:\", decode(generated_seq))\n",
    "    \n",
    "    # Generate 50 characters\n",
    "    for _ in range(50):\n",
    "        probs = forward(generated_seq[-block_size:])\n",
    "        next_id = np.random.choice(len(probs), p=probs)\n",
    "        generated_seq.append(next_id)\n",
    "        \n",
    "    print(\"Generated text:\", decode(generated_seq))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8838945e-8f28-417c-a92c-9a4834103ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct dimensions\n",
    "block_size = 16\n",
    "embedding_dim = 32\n",
    "hidden_dim = 64\n",
    "vocab_size = len(chars)\n",
    "\n",
    "np.random.seed(42)\n",
    "W_embed = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "W1 = np.random.randn(block_size * embedding_dim, hidden_dim) * 0.01  # 16*32 = 512\n",
    "b1 = np.zeros(hidden_dim)\n",
    "W2 = np.random.randn(hidden_dim, vocab_size) * 0.01\n",
    "b2 = np.zeros(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9672bd1e-ad9a-4e46-9e5b-62ad774a6292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(start_seq, length=200, temperature=0.8, top_k=5):\n",
    "    \"\"\"Generate text from start sequence using current model.\"\"\"\n",
    "    generated = start_seq.copy()\n",
    "    \n",
    "    for _ in range(length):\n",
    "        # Take last block_size tokens\n",
    "        context = generated[-block_size:]\n",
    "        \n",
    "        # Forward pass\n",
    "        embeds = W_embed[context].flatten()\n",
    "        h = np.tanh(embeds @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = np.exp(logits) / np.sum(np.exp(logits))  # softmax\n",
    "        \n",
    "        # Apply temperature\n",
    "        probs = probs ** (1/temperature)\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        # Apply top-k filtering\n",
    "        if top_k is not None:\n",
    "            top_idx = np.argsort(probs)[-top_k:]\n",
    "            mask = np.zeros_like(probs)\n",
    "            mask[top_idx] = probs[top_idx]\n",
    "            probs = mask / mask.sum()\n",
    "        \n",
    "        # Sample next token\n",
    "        next_id = np.random.choice(len(probs), p=probs)\n",
    "        generated.append(next_id)\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481d3b8a-a668-488f-a61b-b2d652553487",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "num_epochs = 20  # more epochs for better learning\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(len(x_manual)):\n",
    "        x_seq = x_manual[i]\n",
    "        y_true = y_manual[i]\n",
    "        \n",
    "        # Forward pass\n",
    "        embeds = W_embed[x_seq].flatten()\n",
    "        h = np.tanh(embeds @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "        \n",
    "        # Loss\n",
    "        loss = -np.log(probs[y_true] + 1e-9)\n",
    "        total_loss += loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        dlogits = probs.copy()\n",
    "        dlogits[y_true] -= 1\n",
    "        \n",
    "        dW2 = np.outer(h, dlogits)\n",
    "        db2 = dlogits\n",
    "        \n",
    "        dh = dlogits @ W2.T\n",
    "        dh_raw = dh * (1 - h**2)\n",
    "        \n",
    "        dW1 = np.outer(embeds, dh_raw)\n",
    "        db1 = dh_raw\n",
    "        \n",
    "        dembed_flat = dh_raw @ W1.T\n",
    "        dembed = dembed_flat.reshape(block_size, embedding_dim)\n",
    "        dW_embed = np.zeros_like(W_embed)\n",
    "        for j, idx in enumerate(x_seq):\n",
    "            dW_embed[idx] += dembed[j]\n",
    "        \n",
    "        # Update weights\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "        W_embed -= learning_rate * dW_embed\n",
    "    \n",
    "    avg_loss = total_loss / len(x_manual)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Sample text every 5 epochs\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        start_idx = np.random.randint(0, len(x_manual))\n",
    "        start_seq = x_manual[start_idx]\n",
    "        generated_seq = generate_text(start_seq, length=200, temperature=0.8, top_k=5)\n",
    "        print(\"Sample generation:\")\n",
    "        print(decode(generated_seq))\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57be3aa-78e4-4489-8efe-85ff87f92f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_seq = x_manual[0]\n",
    "embeds = W_embed[x_seq].flatten()\n",
    "print(\"embeds.shape:\", embeds.shape)\n",
    "print(\"W1.shape:\", W1.shape)\n",
    "h = np.tanh(embeds @ W1 + b1)\n",
    "print(\"h.shape:\", h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e405829e-8f2a-4132-967c-e1fd32a5e3da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
