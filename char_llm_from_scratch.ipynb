{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Level Language Model from Scratch\n",
    "A step-by-step implementation of a simple neural language model trained on *Frankenstein* by Mary Shelley.\n",
    "\n",
    "We build everything from scratch using only NumPy — no PyTorch, no TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Imports\n",
    "Import the libraries we'll need throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import math\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Load the Raw Text\n",
    "Read the full text of *Frankenstein* from a local file.\n",
    "We print the total length and a preview of the first 500 characters to confirm it loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 438806\n",
      "The Project Gutenberg eBook of Frankenstein; or, the modern prometheus\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before\n"
     ]
    }
   ],
   "source": [
    "# Load the raw text file (download from Project Gutenberg if you don't have it)\n",
    "with open(\"frankenstein.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Length:\", len(text))\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Clean the Text (Remove Gutenberg Boilerplate)\n",
    "Project Gutenberg texts include legal/licensing text at the start and end.\n",
    "We find the `*** START OF` and `*** END OF` markers and slice out only the actual story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text length: 419409\n",
      "THE PROJECT GUTENBERG EBOOK FRANKENSTEIN; OR, THE MODERN PROMETHEUS ***\n",
      "\n",
      "Frankenstein;\n",
      "\n",
      "or, the Modern Prometheus\n",
      "\n",
      "by Mary Wollstonecraft (Godwin) Shelley\n",
      "\n",
      "\n",
      " CONTENTS\n",
      "\n",
      " Letter 1\n",
      " Letter 2\n",
      " Letter 3\n",
      " Letter 4\n",
      " Chapter 1\n",
      " Chapter 2\n",
      " Chapter 3\n",
      " Chapter 4\n",
      " Chapter 5\n",
      " Chapter 6\n",
      " Chapter 7\n",
      " Chapter 8\n",
      " Chapter 9\n",
      " Chapter 10\n",
      " Chapter 11\n",
      " Chapter 12\n",
      " Chapter 13\n",
      " Chapter 14\n",
      " Chapter 15\n",
      " Chapter 16\n",
      " Chapter 17\n",
      " Chapter 18\n",
      " Chapter 19\n",
      " Chapter 20\n",
      " Chapter 21\n",
      " Chapter 22\n",
      " Chapter 23\n",
      " Chapter 24\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Letter 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_marker = \"*** START OF\"  # Marker indicating where the story begins\n",
    "end_marker = \"*** END OF\"      # Marker indicating where the story ends\n",
    "\n",
    "start = text.find(start_marker)  # Find start index of marker\n",
    "end = text.find(end_marker)      # Find end index of marker\n",
    "\n",
    "# If both markers exist, slice the text to only include the story\n",
    "if start != -1 and end != -1:\n",
    "    text = text[start + len(start_marker):end].strip()  # +len(...) skips the marker itself\n",
    "\n",
    "# Print cleaned text length and first 500 characters\n",
    "print(\"Cleaned text length:\", len(text))\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Build the Character Vocabulary\n",
    "We identify all unique characters in the text and create two mappings:\n",
    "- `stoi` (string-to-integer): maps each character to a unique integer ID\n",
    "- `itos` (integer-to-string): maps each integer ID back to its character\n",
    "\n",
    "These mappings are how we convert between human-readable text and numeric tokens the model can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 84\n",
      "All unique characters: ['\\n', ' ', '!', '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'æ', 'è', 'é', 'ê', 'ô', '—', '‘', '’', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "# Identify all unique characters in the text\n",
    "# 'set(text)' returns only unique characters\n",
    "# 'sorted(...)' ensures a consistent, reproducible order\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# Total number of unique characters (this is our vocabulary size)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Mapping from character -> integer index\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "\n",
    "# Mapping from integer index -> character\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Inspect vocabulary\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"All unique characters:\", chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Encode / Decode Helper Functions\n",
    "Define two helper functions:\n",
    "- `encode(s)`: converts a string into a list of integer token IDs\n",
    "- `decode(l)`: converts a list of integer token IDs back into a string\n",
    "\n",
    "These are the tokenizer for our character-level model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    \"\"\"Convert a string into a list of integer token IDs\"\"\"\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    \"\"\"Convert a list of token IDs back into a string\"\"\"\n",
    "    return ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Print the Character-to-ID Mappings\n",
    "Visualize both mappings so we can see exactly how characters are encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stoi mapping:\n",
      "'\\n':0, ' ':1, '!':2, '(':3, ')':4, '*':5, ',':6, '-':7, '.':8, '0':9, '1':10, '2':11, '3':12, '4':13, '5':14, '6':15, '7':16, '8':17, '9':18, ':':19, ';':20, '?':21, 'A':22, 'B':23, 'C':24, 'D':25, 'E':26, 'F':27, 'G':28, 'H':29, 'I':30, 'J':31, 'K':32, 'L':33, 'M':34, 'N':35, 'O':36, 'P':37, 'R':38, 'S':39, 'T':40, 'U':41, 'V':42, 'W':43, 'Y':44, '[':45, ']':46, '_':47, 'a':48, 'b':49, 'c':50, 'd':51, 'e':52, 'f':53, 'g':54, 'h':55, 'i':56, 'j':57, 'k':58, 'l':59, 'm':60, 'n':61, 'o':62, 'p':63, 'q':64, 'r':65, 's':66, 't':67, 'u':68, 'v':69, 'w':70, 'x':71, 'y':72, 'z':73, 'æ':74, 'è':75, 'é':76, 'ê':77, 'ô':78, '—':79, '‘':80, '’':81, '“':82, '”':83\n",
      "\n",
      "itos mapping:\n",
      "0:\n",
      ", 1: , 2:!, 3:(, 4:), 5:*, 6:,, 7:-, 8:., 9:0, 10:1, 11:2, 12:3, 13:4, 14:5, 15:6, 16:7, 17:8, 18:9, 19::, 20:;, 21:?, 22:A, 23:B, 24:C, 25:D, 26:E, 27:F, 28:G, 29:H, 30:I, 31:J, 32:K, 33:L, 34:M, 35:N, 36:O, 37:P, 38:R, 39:S, 40:T, 41:U, 42:V, 43:W, 44:Y, 45:[, 46:], 47:_, 48:a, 49:b, 50:c, 51:d, 52:e, 53:f, 54:g, 55:h, 56:i, 57:j, 58:k, 59:l, 60:m, 61:n, 62:o, 63:p, 64:q, 65:r, 66:s, 67:t, 68:u, 69:v, 70:w, 71:x, 72:y, 73:z, 74:æ, 75:è, 76:é, 77:ê, 78:ô, 79:—, 80:‘, 81:’, 82:“, 83:”\n"
     ]
    }
   ],
   "source": [
    "# Print stoi mapping (character -> ID)\n",
    "print(\"stoi mapping:\")\n",
    "print(\", \".join([f\"{repr(ch)}:{i}\" for ch, i in stoi.items()]))\n",
    "\n",
    "print()\n",
    "\n",
    "# Print itos mapping (ID -> character)\n",
    "print(\"itos mapping:\")\n",
    "print(\", \".join([f\"{repr(ch)}:{i}\" for ch, i in itos.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Encode the Entire Text\n",
    "Convert the full cleaned text into a list of integer token IDs.\n",
    "Then verify by decoding a small sample back to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 token IDs: [40, 29, 26, 1, 37, 38, 36, 31, 26, 24, 40, 1, 28, 41, 40, 26, 35, 23, 26, 38]\n",
      "Decoded text: 'THE PROJECT GUTENBER'\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire text into a list of integers\n",
    "encoded_text = encode(text)\n",
    "\n",
    "# Sanity check: print first 20 token IDs\n",
    "print(\"First 20 token IDs:\", encoded_text[:20])\n",
    "\n",
    "# Decode them back to verify correctness\n",
    "decoded_sample = decode(encoded_text[:20])\n",
    "print(\"Decoded text:\", repr(decoded_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Visualize Token ID ↔ Character Pairs\n",
    "For the first 20 tokens, print each ID alongside its decoded character.\n",
    "This helps build intuition for how the encoding works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 40 -> 'T'\n",
      "ID 29 -> 'H'\n",
      "ID 26 -> 'E'\n",
      "ID  1 -> ' '\n",
      "ID 37 -> 'P'\n",
      "ID 38 -> 'R'\n",
      "ID 36 -> 'O'\n",
      "ID 31 -> 'J'\n",
      "ID 26 -> 'E'\n",
      "ID 24 -> 'C'\n",
      "ID 40 -> 'T'\n",
      "ID  1 -> ' '\n",
      "ID 28 -> 'G'\n",
      "ID 41 -> 'U'\n",
      "ID 40 -> 'T'\n",
      "ID 26 -> 'E'\n",
      "ID 35 -> 'N'\n",
      "ID 23 -> 'B'\n",
      "ID 26 -> 'E'\n",
      "ID 38 -> 'R'\n",
      "\n",
      "40:'T' | 29:'H' | 26:'E' | 1:' ' | 37:'P' | 38:'R' | 36:'O' | 31:'J' | 26:'E' | 24:'C' | 40:'T' | 1:' ' | 28:'G' | 41:'U' | 40:'T' | 26:'E' | 35:'N' | 23:'B' | 26:'E' | 38:'R'\n"
     ]
    }
   ],
   "source": [
    "# Create a list of tuples: (ID, character)\n",
    "id_char_pairs = [(i, itos[i]) for i in encoded_text[:20]]\n",
    "\n",
    "# Print neatly\n",
    "for idx, char in id_char_pairs:\n",
    "    print(f\"ID {idx:2} -> {repr(char)}\")\n",
    "\n",
    "print()\n",
    "# Also print as a compact inline view\n",
    "print(\" | \".join([f\"{i}:{repr(itos[i])}\" for i in encoded_text[:20]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Create Input-Target Sequences\n",
    "This is where we create training data for the model.\n",
    "\n",
    "**Concept**: Given a window of `block_size` characters (the \"context\"), the model's job is to predict the very next character (the \"target\").\n",
    "\n",
    "We slide this window across the entire text to produce thousands of (input, target) pairs.\n",
    "\n",
    "Example with block_size=8:\n",
    "- Input:  `\"Chapter \"` → Target: `\"1\"`\n",
    "- Input:  `\"hapter 1\"` → Target: `\"\\n\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 419401\n"
     ]
    }
   ],
   "source": [
    "# block_size = number of previous characters the model sees (context window)\n",
    "block_size = 8  # small context window for a toy model\n",
    "\n",
    "# Prepare empty lists to store sequences\n",
    "x_manual = []  # input sequences (lists of integers)\n",
    "y_manual = []  # target characters (next character after input)\n",
    "\n",
    "# Loop over the encoded text to create sequences\n",
    "# We stop at len(encoded_text) - block_size to ensure each input has exactly block_size chars\n",
    "for i in range(len(encoded_text) - block_size):\n",
    "    # Slice the encoded text from i to i+block_size → the \"context\" the model sees\n",
    "    input_seq = encoded_text[i:i+block_size]\n",
    "    x_manual.append(input_seq)\n",
    "    \n",
    "    # The target is the very next character (token ID) after the input sequence\n",
    "    target = encoded_text[i + block_size]\n",
    "    y_manual.append(target)\n",
    "\n",
    "print(f\"Total training examples: {len(x_manual)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Inspect Training Examples\n",
    "Print the first 5 input-target pairs to verify our data preparation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Input IDs:      [40, 29, 26, 1, 37, 38, 36, 31]\n",
      "Decoded Input:  THE PROJ\n",
      "Target ID:      26\n",
      "Decoded Target: E\n",
      "---\n",
      "Example 2\n",
      "Input IDs:      [29, 26, 1, 37, 38, 36, 31, 26]\n",
      "Decoded Input:  HE PROJE\n",
      "Target ID:      24\n",
      "Decoded Target: C\n",
      "---\n",
      "Example 3\n",
      "Input IDs:      [26, 1, 37, 38, 36, 31, 26, 24]\n",
      "Decoded Input:  E PROJEC\n",
      "Target ID:      40\n",
      "Decoded Target: T\n",
      "---\n",
      "Example 4\n",
      "Input IDs:      [1, 37, 38, 36, 31, 26, 24, 40]\n",
      "Decoded Input:   PROJECT\n",
      "Target ID:      1\n",
      "Decoded Target:  \n",
      "---\n",
      "Example 5\n",
      "Input IDs:      [37, 38, 36, 31, 26, 24, 40, 1]\n",
      "Decoded Input:  PROJECT \n",
      "Target ID:      28\n",
      "Decoded Target: G\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(\"Input IDs:     \", x_manual[i])          # token IDs of input\n",
    "    print(\"Decoded Input: \", decode(x_manual[i]))  # convert IDs back to characters\n",
    "    print(\"Target ID:     \", y_manual[i])          # token ID of target\n",
    "    print(\"Decoded Target:\", itos[y_manual[i]])    # convert ID to character\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Initialize Model Parameters\n",
    "We define a simple feedforward neural network with:\n",
    "1. **Embedding layer** (`W_embed`): Converts each token ID into a dense vector of size `embedding_dim`\n",
    "2. **Hidden layer** (`W1`, `b1`): Takes the flattened concatenation of all embeddings and maps to `hidden_dim` neurons with tanh activation\n",
    "3. **Output layer** (`W2`, `b2`): Maps hidden state to logits over the vocabulary (one score per character)\n",
    "\n",
    "All weights are initialized with small random values; biases start at zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (84, 16)\n",
      "Hidden layer weight shape: (128, 32)\n",
      "Output layer weight shape: (32, 84)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for our toy model\n",
    "vocab_size = len(chars)     # number of unique characters\n",
    "embedding_dim = 16          # size of each character embedding vector\n",
    "hidden_dim = 32             # size of hidden layer\n",
    "block_size = 8              # context length (number of previous characters)\n",
    "\n",
    "# Embedding matrix: maps token IDs -> dense vectors\n",
    "# Shape: vocab_size x embedding_dim\n",
    "W_embed = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "\n",
    "# Hidden layer weights: flattened embeddings -> hidden_dim\n",
    "# Input size = block_size * embedding_dim (all embeddings concatenated)\n",
    "W1 = np.random.randn(block_size * embedding_dim, hidden_dim) * 0.01\n",
    "b1 = np.zeros(hidden_dim)  # bias for hidden layer\n",
    "\n",
    "# Output layer: hidden_dim -> vocab_size logits\n",
    "W2 = np.random.randn(hidden_dim, vocab_size) * 0.01\n",
    "b2 = np.zeros(vocab_size)  # bias for output layer\n",
    "\n",
    "# Sanity check: print shapes\n",
    "print(\"Embedding matrix shape:\", W_embed.shape)\n",
    "print(\"Hidden layer weight shape:\", W1.shape)\n",
    "print(\"Output layer weight shape:\", W2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Softmax and Forward Pass Functions\n",
    "**Softmax**: Converts raw logits into a valid probability distribution (all positive, sums to 1).\n",
    "We subtract the max for numerical stability to prevent overflow in `exp()`.\n",
    "\n",
    "**Forward pass**: The full pipeline from input token IDs to output probabilities:\n",
    "1. Look up embeddings for each token\n",
    "2. Flatten all embeddings into one long vector\n",
    "3. Pass through hidden layer with tanh activation\n",
    "4. Compute output logits\n",
    "5. Apply softmax to get probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax probabilities for a 1D array.\n",
    "    Subtracting max(x) prevents numerical overflow.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "def forward(x_seq):\n",
    "    \"\"\"\n",
    "    Forward pass for a single input sequence.\n",
    "    \n",
    "    Args:\n",
    "        x_seq: list of token IDs (length = block_size)\n",
    "    \n",
    "    Returns:\n",
    "        probs: probability distribution over vocab for the next character\n",
    "    \"\"\"\n",
    "    # Step 1: Lookup embeddings for each token ID\n",
    "    embeds = W_embed[x_seq]        # shape: block_size x embedding_dim\n",
    "    \n",
    "    # Step 2: Flatten embeddings into a single vector\n",
    "    h_input = embeds.flatten()     # shape: block_size * embedding_dim\n",
    "    \n",
    "    # Step 3: Hidden layer with tanh activation\n",
    "    h = np.tanh(h_input @ W1 + b1) # shape: hidden_dim\n",
    "    \n",
    "    # Step 4: Output layer -> logits (one score per vocab character)\n",
    "    logits = h @ W2 + b2           # shape: vocab_size\n",
    "    \n",
    "    # Step 5: Softmax -> probabilities\n",
    "    probs = softmax(logits)\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 13: Test the Forward Pass (Before Training)\n",
    "Run the model on the first training example to see what it predicts *before* any training.\n",
    "Since weights are random, the prediction will be essentially random too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence (decoded): THE PROJ\n",
      "Target character: E\n",
      "Predicted character (before training): 0\n",
      "\n",
      "Top 5 predictions:\n",
      "  '0': 0.0119\n",
      "  'x': 0.0119\n",
      "  '4': 0.0119\n",
      "  '1': 0.0119\n",
      "  'é': 0.0119\n"
     ]
    }
   ],
   "source": [
    "# Take first input sequence and its target\n",
    "x0 = x_manual[0]\n",
    "y0 = y_manual[0]\n",
    "\n",
    "# Forward pass: get predicted probability distribution\n",
    "probs = forward(x0)\n",
    "\n",
    "# Predicted character = highest probability\n",
    "pred_id = np.argmax(probs)\n",
    "\n",
    "# Print input, target, and prediction\n",
    "print(\"Input sequence (decoded):\", decode(x0))\n",
    "print(\"Target character:\", itos[y0])\n",
    "print(\"Predicted character (before training):\", itos[pred_id])\n",
    "\n",
    "# Show probabilities of top 5 characters\n",
    "top5 = np.argsort(probs)[-5:][::-1]\n",
    "print(\"\\nTop 5 predictions:\")\n",
    "for i in top5:\n",
    "    print(f\"  {repr(itos[i])}: {probs[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 14: Cross-Entropy Loss Function\n",
    "The loss function measures how far off the model's predictions are from the true answer.\n",
    "\n",
    "**Cross-entropy loss** = `-log(predicted probability of the correct character)`\n",
    "- If the model assigns probability 1.0 to the correct character → loss = 0 (perfect)\n",
    "- If the model assigns probability 0.01 → loss ≈ 4.6 (very bad)\n",
    "\n",
    "We add a tiny epsilon (1e-9) to prevent `log(0)` which would be -infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(probs, target_id):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss for a single prediction.\n",
    "    \n",
    "    Args:\n",
    "        probs: probability distribution over vocab (output of forward pass)\n",
    "        target_id: integer ID of the correct next character\n",
    "    \n",
    "    Returns:\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    return -np.log(probs[target_id] + 1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 15: Training Loop (First Pass — Small Model)\n",
    "Train the model using **stochastic gradient descent (SGD)** with manual backpropagation.\n",
    "\n",
    "For each training example:\n",
    "1. **Forward pass**: Compute predictions\n",
    "2. **Compute loss**: Cross-entropy between prediction and target\n",
    "3. **Backpropagation**: Compute gradients of loss w.r.t. every weight\n",
    "4. **Update weights**: Nudge each weight in the direction that reduces loss\n",
    "\n",
    "We also generate sample text periodically to see the model's progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average Loss: 3.5390\n",
      "Sample start: ished he\n",
      "Generated text: ished hed bhosm\n",
      "FMongroskbree badtel.e ase andtwpos thosse\n",
      "---\n",
      "Epoch 2/5, Average Loss: 3.5576\n",
      "Sample start: ecution \n",
      "Generated text: ecution \n",
      "an wyssalttheqn anexlinb Ht acdentannd. ind doses\n",
      "---\n",
      "Epoch 3/5, Average Loss: 3.5335\n",
      "Sample start: vouring \n",
      "Generated text: vouring lantandw! an wast th SH\n",
      "ann.ensons ,syekd Bnstinkw\n",
      "---\n",
      "Epoch 4/5, Average Loss: 3.5533\n",
      "Sample start: ith bitt\n",
      "Generated text: ith bitterttind andtan\n",
      ",awtan.\n",
      "ans\n",
      "lastancs an ly tantangt\n",
      "---\n",
      "Epoch 5/5, Average Loss: 3.5455\n",
      "Sample start: t, then,\n",
      "Generated text: t, then, oaes annaajs anerl ln cas I ankth in asrant lnyam\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.1       # step size for gradient descent\n",
    "num_epochs = 5            # number of passes over the dataset\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Loop over each input-target pair\n",
    "    for i in range(len(x_manual)):\n",
    "        x_seq = x_manual[i]    # input sequence of token IDs\n",
    "        y_true = y_manual[i]   # target token ID\n",
    "        \n",
    "        # ---- Forward Pass ----\n",
    "        embeds = W_embed[x_seq]                  # Lookup embeddings: block_size x embedding_dim\n",
    "        h_input = embeds.flatten()               # Flatten to single vector\n",
    "        h = np.tanh(h_input @ W1 + b1)           # Hidden layer with tanh activation\n",
    "        logits = h @ W2 + b2                     # Output logits\n",
    "        probs = softmax(logits)                  # Probabilities over vocab\n",
    "        \n",
    "        # ---- Compute Loss (Cross-Entropy) ----\n",
    "        loss = cross_entropy_loss(probs, y_true)\n",
    "        total_loss += loss\n",
    "        \n",
    "        # ---- Backpropagation ----\n",
    "        # Gradient of loss w.r.t logits (softmax + cross-entropy combined)\n",
    "        dlogits = probs.copy()\n",
    "        dlogits[y_true] -= 1  # subtract 1 at the true class\n",
    "        \n",
    "        # Gradients for output layer\n",
    "        dW2 = np.outer(h, dlogits)   # hidden_dim x vocab_size\n",
    "        db2 = dlogits                # vocab_size\n",
    "        \n",
    "        # Gradients for hidden layer\n",
    "        dh = dlogits @ W2.T                    # propagate gradient back\n",
    "        dh_raw = dh * (1 - h**2)               # tanh derivative: 1 - tanh(x)^2\n",
    "        \n",
    "        dW1 = np.outer(h_input, dh_raw)        # input_dim x hidden_dim\n",
    "        db1 = dh_raw                           # hidden_dim\n",
    "        \n",
    "        # Gradients for embeddings\n",
    "        dembed_flat = dh_raw @ W1.T            # block_size * embedding_dim\n",
    "        dembed = dembed_flat.reshape(block_size, embedding_dim)\n",
    "        dW_embed = np.zeros_like(W_embed)\n",
    "        for j, idx in enumerate(x_seq):\n",
    "            dW_embed[idx] += dembed[j]         # accumulate grads for repeated tokens\n",
    "        \n",
    "        # ---- Update Weights (SGD) ----\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "        W_embed -= learning_rate * dW_embed\n",
    "        \n",
    "    # End of epoch: print average loss and generate sample text\n",
    "    avg_loss = total_loss / len(x_manual)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Generate a sample of 50 characters\n",
    "    start_idx = np.random.randint(0, len(x_manual))\n",
    "    generated_seq = x_manual[start_idx].copy()\n",
    "    print(\"Sample start:\", decode(generated_seq))\n",
    "    \n",
    "    for _ in range(50):\n",
    "        probs = forward(generated_seq[-block_size:])\n",
    "        next_id = np.random.choice(len(probs), p=probs)\n",
    "        generated_seq.append(next_id)\n",
    "        \n",
    "    print(\"Generated text:\", decode(generated_seq))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 16: Scale Up — Larger Model with Longer Context\n",
    "Now we increase the model capacity:\n",
    "- `block_size`: 8 → 16 (sees more context)\n",
    "- `embedding_dim`: 16 → 32 (richer character representations)\n",
    "- `hidden_dim`: 32 → 64 (more expressive hidden layer)\n",
    "\n",
    "We also need to **rebuild the training data** with the new block_size,\n",
    "and reinitialize all weights to match the new dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 419393 (with block_size=16)\n",
      "W_embed: (84, 32), W1: (512, 64), W2: (64, 84)\n"
     ]
    }
   ],
   "source": [
    "# New hyperparameters\n",
    "block_size = 16\n",
    "embedding_dim = 32\n",
    "hidden_dim = 64\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Rebuild training data with new block_size\n",
    "x_manual = []\n",
    "y_manual = []\n",
    "for i in range(len(encoded_text) - block_size):\n",
    "    x_manual.append(encoded_text[i:i+block_size])\n",
    "    y_manual.append(encoded_text[i + block_size])\n",
    "\n",
    "print(f\"Training examples: {len(x_manual)} (with block_size={block_size})\")\n",
    "\n",
    "# Reinitialize weights with proper dimensions\n",
    "np.random.seed(42)\n",
    "W_embed = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "W1 = np.random.randn(block_size * embedding_dim, hidden_dim) * 0.01  # 16*32 = 512 inputs\n",
    "b1 = np.zeros(hidden_dim)\n",
    "W2 = np.random.randn(hidden_dim, vocab_size) * 0.01\n",
    "b2 = np.zeros(vocab_size)\n",
    "\n",
    "print(f\"W_embed: {W_embed.shape}, W1: {W1.shape}, W2: {W2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 17: Text Generation Function\n",
    "A reusable function to generate text from the trained model.\n",
    "\n",
    "Key generation controls:\n",
    "- **Temperature**: Controls randomness. Lower = more conservative (picks common characters), Higher = more creative/risky\n",
    "- **Top-k filtering**: Only sample from the top k most likely characters, zeroing out the rest. Prevents very unlikely characters from being chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(start_seq, length=200, temperature=0.8, top_k=5):\n",
    "    \"\"\"Generate text from a starting sequence using the current model.\n",
    "    \n",
    "    Args:\n",
    "        start_seq: list of token IDs to start from (at least block_size long)\n",
    "        length: number of characters to generate\n",
    "        temperature: controls randomness (lower = more deterministic)\n",
    "        top_k: only sample from top k most likely characters\n",
    "    \n",
    "    Returns:\n",
    "        Full generated sequence (start + generated) as list of token IDs\n",
    "    \"\"\"\n",
    "    generated = start_seq.copy()\n",
    "    \n",
    "    for _ in range(length):\n",
    "        # Take the last block_size tokens as context\n",
    "        context = generated[-block_size:]\n",
    "        \n",
    "        # Forward pass\n",
    "        embeds = W_embed[context].flatten()\n",
    "        h = np.tanh(embeds @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = np.exp(logits) / np.sum(np.exp(logits))  # softmax\n",
    "        \n",
    "        # Apply temperature scaling (raise probs to power 1/T, then renormalize)\n",
    "        probs = probs ** (1 / temperature)\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        # Apply top-k filtering: keep only top k probabilities\n",
    "        if top_k is not None:\n",
    "            top_idx = np.argsort(probs)[-top_k:]\n",
    "            mask = np.zeros_like(probs)\n",
    "            mask[top_idx] = probs[top_idx]\n",
    "            probs = mask / mask.sum()\n",
    "        \n",
    "        # Sample the next token from the probability distribution\n",
    "        next_id = np.random.choice(len(probs), p=probs)\n",
    "        generated.append(next_id)\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 18: Train the Larger Model (20 Epochs)\n",
    "Train the scaled-up model for 20 epochs with the same SGD + backpropagation approach.\n",
    "Every 5 epochs, we generate a 200-character sample to monitor quality.\n",
    "\n",
    "**Note**: This will take a while since we're training on the full text with pure NumPy (no GPU).\n",
    "The loss should decrease over epochs, and generated text should become more English-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 5.3722\n",
      "Epoch 2/20, Loss: 5.4459\n",
      "Epoch 3/20, Loss: 5.4606\n",
      "Epoch 4/20, Loss: 5.4705\n",
      "Epoch 5/20, Loss: 5.4701\n",
      "Sample generation:\n",
      "hero, whose extian thi leytay\n",
      " lT wao was ano ens ano adn bn erdtpan wn  bo  aystaosihhir\n",
      "e wasem bn thb vrs ane lns bls ai waat an who te  pau ansthagiwn ahae thoe;cnn.lgn tls aistut.t ahl ans toct.tsia. wtes bhtt.a\n",
      "---\n",
      "Epoch 6/20, Loss: 5.4809\n",
      "Epoch 7/20, Loss: 5.4786\n",
      "Epoch 8/20, Loss: 5.4636\n",
      "Epoch 9/20, Loss: 5.4820\n",
      "Epoch 10/20, Loss: 5.4908\n",
      "Sample generation:\n",
      "he productions oe thslinte.ne wass yc wh\n",
      " wa sooeth\n",
      " whstaycewe shalebhenhw.n  eonst\n",
      "i weseiardnh bn\n",
      " w st nk Hre ta t st stno a sdin saio emisdectesce se m stes\n",
      " wne he sase yhsonn wd sestan tn eristiltedf\n",
      "nhaoh\n",
      "teo\n",
      "---\n",
      "Epoch 11/20, Loss: 5.4929\n",
      "Epoch 12/20, Loss: 5.5137\n",
      "Epoch 13/20, Loss: 5.5134\n",
      "Epoch 14/20, Loss: 5.4951\n",
      "Epoch 15/20, Loss: 5.4865\n",
      "Sample generation:\n",
      "yourself. And yesededn.ank. cokere td\n",
      "cl ttstee s des tesn nsr srd uoteatlwa\n",
      "r nardlnse lodnalhfy\n",
      "thtnresetld He tkdt shrnselirrs\n",
      ".e and. sotnse.na\n",
      " blalethbcdetoses\n",
      "\n",
      "not dndese to\n",
      "cvnt dhedaaldnwneHn sert Ihrh\n",
      "noaso\n",
      "---\n",
      "Epoch 16/20, Loss: 5.4720\n",
      "Epoch 17/20, Loss: 5.4608\n",
      "Epoch 18/20, Loss: 5.4522\n",
      "Epoch 19/20, Loss: 5.4582\n",
      "Epoch 20/20, Loss: 5.4574\n",
      "Sample generation:\n",
      "ent of a nervousn\n",
      " ca t ot nhet; nn\n",
      " faitrithdt dne weota cas\n",
      " thetw nn\n",
      "nc cas\n",
      "n ntewe ofv\n",
      " n t d apvn,ls.nd the nestli.n tee t strnse s sna ces wlst thende nm caitpess.ntn thit inxmhas.tne n.\n",
      "e bnasli\n",
      "et seot inn\n",
      "u \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(len(x_manual)):\n",
    "        x_seq = x_manual[i]\n",
    "        y_true = y_manual[i]\n",
    "        \n",
    "        # ---- Forward pass ----\n",
    "        embeds = W_embed[x_seq].flatten()\n",
    "        h = np.tanh(embeds @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = np.exp(logits) / np.sum(np.exp(logits))  # softmax\n",
    "        \n",
    "        # ---- Loss ----\n",
    "        loss = -np.log(probs[y_true] + 1e-9)\n",
    "        total_loss += loss\n",
    "        \n",
    "        # ---- Backpropagation ----\n",
    "        dlogits = probs.copy()\n",
    "        dlogits[y_true] -= 1\n",
    "        \n",
    "        dW2 = np.outer(h, dlogits)\n",
    "        db2 = dlogits\n",
    "        \n",
    "        dh = dlogits @ W2.T\n",
    "        dh_raw = dh * (1 - h**2)  # tanh derivative\n",
    "        \n",
    "        dW1 = np.outer(embeds, dh_raw)\n",
    "        db1 = dh_raw\n",
    "        \n",
    "        dembed_flat = dh_raw @ W1.T\n",
    "        dembed = dembed_flat.reshape(block_size, embedding_dim)\n",
    "        dW_embed = np.zeros_like(W_embed)\n",
    "        for j, idx in enumerate(x_seq):\n",
    "            dW_embed[idx] += dembed[j]\n",
    "        \n",
    "        # ---- Update weights (SGD) ----\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "        W_embed -= learning_rate * dW_embed\n",
    "    \n",
    "    # Print average loss for this epoch\n",
    "    avg_loss = total_loss / len(x_manual)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Generate sample text every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        start_idx = np.random.randint(0, len(x_manual))\n",
    "        start_seq = x_manual[start_idx]\n",
    "        generated_seq = generate_text(start_seq, length=200, temperature=0.8, top_k=5)\n",
    "        print(\"Sample generation:\")\n",
    "        print(decode(generated_seq))\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 19: Debug — Verify Tensor Shapes\n",
    "A quick sanity check to confirm all intermediate tensors have the expected shapes.\n",
    "This is essential for catching dimension mismatches that cause cryptic errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeds.shape: (512,)\n",
      "W1.shape: (512, 64)\n",
      "h.shape: (64,)\n",
      "logits.shape: (84,)\n",
      "\n",
      "All shapes look correct!\n"
     ]
    }
   ],
   "source": [
    "# Verify shapes through the forward pass\n",
    "x_seq = x_manual[0]\n",
    "embeds = W_embed[x_seq].flatten()\n",
    "print(\"embeds.shape:\", embeds.shape)      # Expected: (block_size * embedding_dim,) = (512,)\n",
    "print(\"W1.shape:\", W1.shape)              # Expected: (512, 64)\n",
    "\n",
    "h = np.tanh(embeds @ W1 + b1)\n",
    "print(\"h.shape:\", h.shape)                # Expected: (64,)\n",
    "\n",
    "logits = h @ W2 + b2\n",
    "print(\"logits.shape:\", logits.shape)       # Expected: (vocab_size,)\n",
    "print(\"\\nAll shapes look correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 20: Generate Final Sample\n",
    "Use the fully trained model to generate a longer passage of text.\n",
    "Try different temperature and top_k values to see how they affect output quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL GENERATION (temperature=0.8, top_k=5)\n",
      "============================================================\n",
      "foe. If I were an che\n",
      "de sais ileentrnt o de ekitnn\n",
      "n\n",
      "otr tist tce\n",
      "n Unane st spidtrlirovtheest ct tai\n",
      "adhcanmrna f s adsesw nn\n",
      "fe masoees.a,na dae woswln d d a seawcocrn\n",
      " y se the\n",
      "we nadt n.vtnse s s iedstn..ndne them;e nt chispl neot sy sle nesthi.fidae t sisoses.so sne t stan\n",
      "e.\n",
      "elit theest,n s d e wesnroct s l \n",
      "\n",
      "============================================================\n",
      "LOWER TEMPERATURE (temperature=0.5, top_k=5) — more conservative\n",
      "============================================================\n",
      "foe. If I were an chisastn nnd cnot in,s; cfe nest nonesisn nnase hnrn boatlruns nn t slnesecnt n\n",
      " t son nesnc cf deest snr ciete snst nct ha\n",
      "nsdce.\n",
      "no tee to\n",
      "thitrs\n",
      "icnt ncst nlthd s s sistuwcrsone e so nae.n nn\n",
      "nc c\n",
      "a\n",
      " aneew rn\n",
      "pc teaween.rh r s wese.”n\n",
      "o ne saeswese\n",
      "do n\n",
      " tnese.s.rive tee.w bna nastawcrn\n",
      "r naata\n",
      "\n",
      "============================================================\n",
      "HIGHER TEMPERATURE (temperature=1.2, top_k=10) — more creative\n",
      "============================================================\n",
      "foe. If I were a.” indttrsfn\n",
      "ne sa sae\n",
      "wustr .ao libtdn.e.vnanm yle chrt no whd\n",
      "n nerwerot in darisesnw..nl nide th Tn dnesfrhnrd uee wnitaosib\n",
      "dene wa a nkess nnrncv.hok rne wlat bkrsteserdnv,a saacetsibn bn tedt,bhys miee ine\n",
      "n nnancicafoctht\n",
      "si nyeelsus\n",
      " r itest,.!p n  maiweswsbn bhv se ceiSf”splo rhv ,pdeebs!n \n"
     ]
    }
   ],
   "source": [
    "# Pick a random starting context from the training data\n",
    "start_idx = np.random.randint(0, len(x_manual))\n",
    "start_seq = x_manual[start_idx]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL GENERATION (temperature=0.8, top_k=5)\")\n",
    "print(\"=\" * 60)\n",
    "generated = generate_text(start_seq, length=300, temperature=0.8, top_k=5)\n",
    "print(decode(generated))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOWER TEMPERATURE (temperature=0.5, top_k=5) — more conservative\")\n",
    "print(\"=\" * 60)\n",
    "generated = generate_text(start_seq, length=300, temperature=0.5, top_k=5)\n",
    "print(decode(generated))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HIGHER TEMPERATURE (temperature=1.2, top_k=10) — more creative\")\n",
    "print(\"=\" * 60)\n",
    "generated = generate_text(start_seq, length=300, temperature=1.2, top_k=10)\n",
    "print(decode(generated))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
